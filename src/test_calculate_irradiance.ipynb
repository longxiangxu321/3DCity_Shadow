{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pvlib\n",
    "from joblib import Parallel, delayed\n",
    "import logging\n",
    "import time\n",
    "import threading\n",
    "import contextlib\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "# Set up basic configuration for logging\n",
    "os.chdir('../')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "with open('./config.json', 'r') as file:\n",
    "    CFG = json.load(file)\n",
    "\n",
    "def read_result(CFG):\n",
    "    data_root = os.path.join(CFG['study_area']['data_root'], \n",
    "                             CFG['shadow_calc']['output_folder_name'])\n",
    "    bin_file = os.path.join(data_root, 'shadow_result','results.bin')\n",
    "    grid_file = os.path.join(data_root, 'intermediate', 'grid.xyz')\n",
    "    gmlid_file = os.path.join(data_root, 'shadow_result','gmlids.csv')\n",
    "    sunpos_file = os.path.join(data_root, 'intermediate','sun_pos.csv')\n",
    "\n",
    "    with open(bin_file, \"rb\") as file:\n",
    "        result_bin = np.fromfile(file, dtype=np.int32)\n",
    "        num_rows = result_bin[0]\n",
    "        num_cols = result_bin[1]  \n",
    "        # Remove the first two elements (number of rows and columns)\n",
    "        result_bin = result_bin[2:]\n",
    "\n",
    "        result_bin = result_bin.reshape((num_rows, num_cols))\n",
    "\n",
    "    point_grid = []\n",
    "    with open(grid_file, 'r') as file:\n",
    "        for line in file:\n",
    "            x, y, z, nx, ny, nz = line.split()\n",
    "            point_grid.append([float(x), float(y), float(z), float(nx), float(ny), float(nz)])\n",
    "\n",
    "    point_grid = np.array(point_grid)\n",
    "\n",
    "    gmlids = pd.read_csv(gmlid_file, header=None)\n",
    "    gmlids = gmlids[[0]]\n",
    "    gmlids.columns = ['gmlid']\n",
    "    sunpos = pd.read_csv(sunpos_file)\n",
    "    sunpos.columns.values[0]='timestamp'\n",
    "    gmlid_array = gmlids.to_numpy()\n",
    "\n",
    "    assert result_bin.shape[0] == point_grid.shape[0]==gmlids.shape[0]\n",
    "    assert result_bin.shape[1] == sunpos.shape[0]\n",
    "    print(\"Num of faces: \", gmlids['gmlid'].nunique())\n",
    "    print(\"Num of sample points: \", point_grid.shape[0])\n",
    "    print(\"Num of timestamps: \", result_bin.shape[1])\n",
    "    return result_bin, point_grid, gmlid_array, sunpos\n",
    "\n",
    "def normal2angle(normal_vector):\n",
    "\n",
    "    normal_vector = np.array(normal_vector)\n",
    "    magnitude = np.linalg.norm(normal_vector)\n",
    "    nx, ny, nz = normal_vector / magnitude\n",
    "    tilt = np.degrees(np.arccos(nz))\n",
    "    azimuth = np.degrees(np.arctan2(nx, ny))\n",
    "    if azimuth < 0:\n",
    "        azimuth += 360\n",
    "\n",
    "    return tilt, azimuth\n",
    "\n",
    "def aggregate_shadow(gmlids, merged_result):\n",
    "    \"\"\"\n",
    "    Aggregate the shadow results by gmlid\n",
    "    gmlids: array of gmlids\n",
    "    merged_result: array of shadow results,     \n",
    "        merged_result[:, -6:-3] are the xyz coordinates\n",
    "        merged_result[:, -3:] are the normal vectors\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "\n",
    "    start_idx = 0\n",
    "    unique_gmlids = []\n",
    "    unique_gmlids.append(gmlids[0])\n",
    "    for i in range(1, len(gmlids)):\n",
    "        if gmlids[i] != gmlids[i - 1]:\n",
    "            unique_gmlids.append(gmlids[i])\n",
    "            end_idx = i\n",
    "            indices.append((start_idx, end_idx))\n",
    "            start_idx = i\n",
    "    # Add the last group\n",
    "    indices.append((start_idx, len(gmlids)))\n",
    "\n",
    "    # Initialize an array to store the aggregated results\n",
    "    aggregated_data = np.empty((len(indices), merged_result.shape[1]))\n",
    "\n",
    "    # Calculate averages for each group\n",
    "    for i, (start, end) in enumerate(indices):\n",
    "        aggregated_data[i, :] = np.mean(merged_result[start:end, :], axis=0)\n",
    "\n",
    "    # breakpoint()\n",
    "    # result_array = np.hstack((aggregated_data[:, :-1], aggregated_data[:, -3:]))\n",
    "    return aggregated_data, unique_gmlids\n",
    "\n",
    "\n",
    "def obtain_tmy(latitude, longitude, sunpos):\n",
    "    tmy = pvlib.iotools.get_pvgis_tmy(latitude, longitude)\n",
    "    irradiance_data = tmy[0]\n",
    "\n",
    "    irradiance_data.index = pd.to_datetime(irradiance_data.index)\n",
    "    tmy = pvlib.iotools.get_pvgis_tmy(latitude, longitude)\n",
    "    irradiance_data = tmy[0]\n",
    "\n",
    "    irradiance_data.index = pd.to_datetime(irradiance_data.index)\n",
    "\n",
    "    sunpos['timestamp'] = pd.to_datetime(sunpos['timestamp'], utc=True)\n",
    "    all_ghi = []\n",
    "    all_dni = []\n",
    "    all_dhi = []\n",
    "    all_solar_zenith = []\n",
    "    all_solar_azimuth = []\n",
    "    all_dni_extra = []\n",
    "\n",
    "    for index, row in sunpos.iterrows():\n",
    "            row_time = row['timestamp'].tz_convert(irradiance_data.index.tz)\n",
    "            month, day, hour = row_time.month, row_time.day, row_time.hour\n",
    "            match = irradiance_data[(irradiance_data.index.month == month) & \n",
    "                        (irradiance_data.index.day == day) & \n",
    "                        (irradiance_data.index.hour == hour)]\n",
    "            solar_zenith = row['apparent_zenith']\n",
    "            solar_azimuth = row['azimuth']\n",
    "            dni_extra = pvlib.irradiance.get_extra_radiation(row['timestamp'])\n",
    "            if (not match.empty):\n",
    "                ghi, dni, dhi = match.iloc[0][['ghi', 'dni', 'dhi']]\n",
    "                all_ghi.append(ghi)\n",
    "                all_dni.append(dni)\n",
    "                all_dhi.append(dhi)\n",
    "                all_solar_zenith.append(solar_zenith)\n",
    "                all_solar_azimuth.append(solar_azimuth)\n",
    "                all_dni_extra.append(dni_extra)\n",
    "            else:\n",
    "                print(\"error finding match\")\n",
    "    \n",
    "    return all_ghi, all_dni, all_dhi, all_solar_zenith, all_solar_azimuth, all_dni_extra\n",
    "\n",
    "\n",
    "def calculate_irradiance(result_array, all_ghi, all_dni, all_dhi, all_solar_zenith, all_solar_azimuth):\n",
    "    \"\"\"\n",
    "    calculate the irradiance for each point in the result_array\n",
    "    result_array: array of shadow results,\n",
    "    # result_array.shape[0] are the number of surfaces\n",
    "    # result_array[:, -6:-3] are xyz coordinates\n",
    "    # result_array[:, -3:] are the normal vectors\n",
    "    \"\"\"\n",
    "    result_arr = np.zeros(result_array.shape, dtype=float)\n",
    "    directs = np.zeros(result_array.shape, dtype=float)\n",
    "    diffuses = np.zeros(result_array.shape, dtype=float)\n",
    "\n",
    "    for i in range(result_array.shape[0]):\n",
    "        # point_result = []\n",
    "        surface_normal = result_array[i][-3:]\n",
    "        surface_tilt, surface_azimuth = normal2angle(surface_normal)\n",
    "        for j in range(len(all_ghi)):\n",
    "                poa = pvlib.irradiance.get_total_irradiance(surface_tilt, surface_azimuth, \n",
    "                                                            all_solar_zenith[j],\n",
    "                                                            all_solar_azimuth[j], \n",
    "                                                            all_dni[j], all_ghi[j], all_dhi[j])\n",
    "                direct = poa['poa_global']\n",
    "                diffuse = poa['poa_diffuse']\n",
    "                directs[i][j] = direct\n",
    "                diffuses[i][j] = diffuse\n",
    "\n",
    "    return result_arr, directs, diffuses\n",
    "\n",
    "def calculate_single_surface(i, result_array, all_ghi, all_dni, all_dhi, all_solar_zenith, all_solar_azimuth, all_dni_extra, all_airmass, model):\n",
    "    \"\"\"\n",
    "    Calculates irradiance for a single surface, to be used in parallel processing.\n",
    "    \"\"\"\n",
    "    surface_normal = result_array[i][-3:]\n",
    "    surface_tilt, surface_azimuth = normal2angle(surface_normal)\n",
    "    directs_local = np.zeros(len(all_ghi))\n",
    "    diffuses_local = np.zeros(len(all_ghi))\n",
    "    \n",
    "    for j in range(len(all_ghi)):\n",
    "        \n",
    "        poa = pvlib.irradiance.get_total_irradiance(surface_tilt, surface_azimuth, \n",
    "                                                    all_solar_zenith[j],\n",
    "                                                    all_solar_azimuth[j], \n",
    "                                                    all_dni[j]+1e-10, all_ghi[j]+1e-10, all_dhi[j]+1e-10, dni_extra=all_dni_extra[j], airmass=all_airmass[j], model=model)\n",
    "        direct = poa['poa_global']\n",
    "        diffuse = poa['poa_diffuse']\n",
    "            \n",
    "        directs_local[j] = direct\n",
    "        diffuses_local[j] = diffuse\n",
    "    \n",
    "    return directs_local, diffuses_local\n",
    "\n",
    "def calculate_irradiance_parallel(result_array, all_ghi, all_dni, all_dhi, all_solar_zenith, all_solar_azimuth, all_dni_extra, all_airmass, model):\n",
    "    \"\"\"\n",
    "    Parallelized calculation of the irradiance for each point in the result_array.\n",
    "    \"\"\"\n",
    "\n",
    "    num_surfaces = result_array.shape[0]\n",
    "    directs = np.zeros((num_surfaces, len(all_ghi)))\n",
    "    diffuses = np.zeros((num_surfaces, len(all_ghi)))\n",
    "    \n",
    "    with tqdm_joblib(tqdm(desc=\"My calculation\", total=num_surfaces)) as progress_bar:\n",
    "        results = Parallel(n_jobs=CFG['num_threads'])(delayed(calculate_single_surface)(i, result_array, all_ghi, all_dni, all_dhi, \n",
    "                                                                                        all_solar_zenith, all_solar_azimuth, all_dni_extra, all_airmass, model) for i in range(num_surfaces))\n",
    "    \n",
    "    for i, (directs_local, diffuses_local) in enumerate(results):\n",
    "        directs[i] = directs_local\n",
    "        diffuses[i] = diffuses_local\n",
    "\n",
    "    return directs, diffuses\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Context manager to patch joblib to report into tqdm progress bar given as argument\"\"\"\n",
    "    # reference: https://stackoverflow.com/questions/24983493/tracking-progress-of-joblib-parallel-execution\n",
    "    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):\n",
    "        def __call__(self, *args, **kwargs):\n",
    "            tqdm_object.update(n=self.batch_size)\n",
    "            return super().__call__(*args, **kwargs)\n",
    "\n",
    "    old_batch_callback = joblib.parallel.BatchCompletionCallBack\n",
    "    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.BatchCompletionCallBack = old_batch_callback\n",
    "        tqdm_object.close()\n",
    "\n",
    "def get_airmass(all_solar_zenith):\n",
    "    airmass = []\n",
    "    for zenith in all_solar_zenith:\n",
    "        airmass.append(pvlib.atmosphere.get_relative_airmass(zenith))\n",
    "    return airmass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_bin, point_grid, gmlids, sunpos = read_result(CFG)\n",
    "print(\"Reading completed\")\n",
    "latitude = CFG['study_area']['lat']\n",
    "longitude = CFG['study_area']['long']\n",
    "\n",
    "merged_result = np.hstack((result_bin, point_grid))\n",
    "# merged_result[:, -6:-3] are the normal vectors\n",
    "# merged_result[:, -3:-1] are the gmlids\n",
    "\n",
    "print(\"aggregating shadow results by gmlid...\")\n",
    "result_array, unique_gmlids = aggregate_shadow(gmlids, merged_result)\n",
    "# result_array[:, -6:-3] are xyz coordinates\n",
    "# result_array[:, -3:] are the normal vectors\n",
    "\n",
    "print(\"Obtaining and processing tmy data...\")\n",
    "all_ghi, all_dni, all_dhi, all_solar_zenith, all_solar_azimuth, all_dni_extra = obtain_tmy(latitude, longitude, sunpos)\n",
    "all_airmass = get_airmass(all_solar_zenith)\n",
    "\n",
    "\n",
    "print(\"Calculating irradiance in parallel with {} threads...\".format(CFG['num_threads']))\n",
    "model = CFG['irradiance_model']\n",
    "print(\"using \" + model+ \" model\")\n",
    "directs, diffuses = calculate_irradiance_parallel(result_array, \n",
    "                                                                all_ghi, all_dni, all_dhi, \n",
    "                                                                all_solar_zenith, all_solar_azimuth, all_dni_extra, all_airmass, model)\n",
    "\n",
    "# directs, diffuses have the same shape\n",
    "shadow_arr = result_array[:, :-6]\n",
    "total_irradiance = directs * shadow_arr  + diffuses\n",
    "\n",
    "unique_gmlids_arr = np.array(unique_gmlids)\n",
    "\n",
    "save_path = os.path.join(CFG['study_area']['data_root'], CFG['shadow_calc']['output_folder_name'],\n",
    "                            model+'_hourly_result.npz')\n",
    "\n",
    "\n",
    "np.savez(save_path, gmlids=unique_gmlids_arr, hourly_irradiance=total_irradiance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
