{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import db_tools\n",
    "import os,sys\n",
    "from sqlalchemy import text, update, Table, MetaData, insert, select, func, delete, bindparam, and_\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "from sqlalchemy.exc import SAWarning\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './citysim/CitySimResults_Utrecht-10-490-596_SW.csv'\n",
    "folder = \"../data/Utrecht/output_20240328_155602\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "citysim_data = pd.read_csv(file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = citysim_data.groupby('surface_gmlid')\n",
    "aggregations = {col: 'first' for col in citysim_data.columns[:5]}  # Take first for the first four columns\n",
    "aggregations.update({col: 'mean' for col in citysim_data.columns[5:]})  # Average for the rest\n",
    "\n",
    "# Perform the aggregation with adjusted strategy\n",
    "result_corrected = grouped.agg(aggregations)\n",
    "\n",
    "citysim_data = result_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\55241\\AppData\\Local\\Temp\\ipykernel_25344\\2863305370.py:4: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise a warning unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  sun_pos['timestamp'] = pd.to_datetime(sun_pos['timestamp'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sunpos = os.path.join(folder, \"intermediate/sun_pos.csv\")\n",
    "sun_pos = pd.read_csv(sunpos)\n",
    "sun_pos.columns.values[0]='timestamp'\n",
    "sun_pos['timestamp'] = pd.to_datetime(sun_pos['timestamp'])\n",
    "timestamps_to_keep = set(sun_pos['timestamp'])\n",
    "\n",
    "config_file = os.path.join(folder, \"config.json\")\n",
    "with open(config_file, 'r') as file:\n",
    "    CFG = json.load(file)[\"study_area\"]\n",
    "times = pd.date_range(CFG[\"start_time\"], CFG[\"end_time\"], freq=CFG[\"frequency\"], tz=CFG[\"timezone\"])\n",
    "total_hours = len(citysim_data.columns[5:])  # Number of timestamped columns\n",
    "# citysim_timestamps = times\n",
    "\n",
    "# # Filter the columns of citysim_data based on the timestamps\n",
    "# columns_to_keep = list(citysim_data.columns[:5]) + [col for idx, col in enumerate(citysim_data.columns[5:], start=5)\n",
    "#                                                     if citysim_timestamps[idx - 5] in timestamps_to_keep]\n",
    "\n",
    "# # Create the filtered DataFrame\n",
    "# filtered_citysim_data = citysim_data[columns_to_keep]\n",
    "# monthly_aggregates = filtered_citysim_data.iloc[:, :5].copy()\n",
    "\n",
    "# # Extract months from the filtered timestamps\n",
    "# month_list = [ts.month for ts in timestamps_to_keep]  # List of months corresponding to each timestamp\n",
    "\n",
    "# # Perform the aggregation\n",
    "# for month in range(1, 13):\n",
    "#     # Indices of columns that correspond to the current month\n",
    "#     columns_this_month = [filtered_citysim_data.columns[5:][i] for i, m in enumerate(month_list) if m == month]\n",
    "    \n",
    "#     # Sum the values across these columns if they exist\n",
    "#     if columns_this_month:\n",
    "#         monthly_aggregates[f'Month_{month}'] = filtered_citysim_data[columns_this_month].sum(axis=1)\n",
    "\n",
    "\n",
    "# all_rows_monthly_data = []\n",
    "# # Loop through each row in the DataFrame to sum up data for each month\n",
    "# for index, row in monthly_aggregates.iterrows():\n",
    "#     monthly_data_list = []\n",
    "#     for data in row[5:]:\n",
    "#         monthly_data_list.append(data)\n",
    "#     all_rows_monthly_data.append(monthly_data_list)\n",
    "    \n",
    "# monthly_aggregates['monthly_data'] = all_rows_monthly_data\n",
    "# # Optionally, drop the previous monthly columns if they are no longer needed\n",
    "# monthly_aggregates = monthly_aggregates.drop(columns=monthly_aggregates.columns[5:-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = citysim_data.copy()\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "identifiers = data[['building_gmlid', 'surface_gmlid', 'multisurface_gmlid']]\n",
    "\n",
    "date_range = pd.date_range(start='2023-01-01', periods=8760, freq='H')\n",
    "\n",
    "# Drop the non-numeric columns for hourly data processing\n",
    "hourly_data = data.drop(columns=['building_gmlid', 'surface_gmlid', 'multisurface_gmlid', 'triangle_gmlid', 'area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Transpose the DataFrame to make hours as rows instead of columns\n",
    "hourly_data_transposed = hourly_data.T\n",
    "\n",
    "# Set the datetime as the index of the transposed DataFrame\n",
    "hourly_data_transposed.index = date_range\n",
    "\n",
    "# Sum data monthly in the transposed DataFrame\n",
    "monthly_data_transposed = hourly_data_transposed.resample('M').sum()\n",
    "\n",
    "monthly_data_transposed = monthly_data_transposed.T\n",
    "# Add the building identifiers to the monthly summed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([identifiers, monthly_data_transposed], axis=1)\n",
    "monthly_aggregates = result.copy()\n",
    "\n",
    "all_rows_monthly_data = []\n",
    "# Loop through each row in the DataFrame to sum up data for each month\n",
    "for index, row in monthly_aggregates.iterrows():\n",
    "    monthly_data_list = []\n",
    "    for data in row[3:]:\n",
    "        monthly_data_list.append(data)\n",
    "    all_rows_monthly_data.append(monthly_data_list)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_aggregates['monthly_data'] = all_rows_monthly_data\n",
    "# Optionally, drop the previous monthly columns if they are no longer needed\n",
    "monthly_aggregates = monthly_aggregates.drop(columns=monthly_aggregates.columns[5:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql+psycopg2://postgres:3344carry@127.0.0.1:5432/solar_calc\n",
      "Connection to database solar_calc was successful\n"
     ]
    }
   ],
   "source": [
    "DB_3DCityDB_ConDetails = \"DB_3DCityDB_ConDetails.txt\"\n",
    "db_3dcitydb = db_tools.engineBuilder(DB_3DCityDB_ConDetails)\n",
    "\n",
    "with open(DB_3DCityDB_ConDetails, 'r') as file:\n",
    "    # Read all lines from the file into a list\n",
    "    lines = file.readlines()\n",
    "\n",
    "schema_name = lines[-1]\n",
    "query_db= f'''\n",
    "select id as cityobject_id, gmlid\n",
    "from {schema_name}.cityobject\n",
    "'''\n",
    "\n",
    "bu_gmlid = pd.read_sql(query_db, db_3dcitydb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_aggregates.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(monthly_aggregates, bu_gmlid, left_on = 'surface_gmlid', right_on='gmlid', how='inner')\n",
    "target_df = merged_df[['gmlid', 'monthly_data','cityobject_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data(merged_df, db):\n",
    "    # \n",
    "    # DB_3DCityDB_ConDetails = \"DB_3DCityDB_ConDetails.txt\"\n",
    "    # db_3dcitydb = db_tools.engineBuilder(DB_3DCityDB_ConDetails)\n",
    "    metadata = MetaData()\n",
    "    db_3dcitydb = db\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=SAWarning)\n",
    "        metadata.reflect(bind=db_3dcitydb)\n",
    "        \n",
    "    cityobject_table = Table('cityobject', metadata, autoload_with=db_3dcitydb)\n",
    "    ng_weatherdata_table = Table('ng_weatherdata', metadata, autoload_with=db_3dcitydb)\n",
    "    ng_timeseries_table = Table('ng_timeseries', metadata, autoload_with=db_3dcitydb)\n",
    "    ng_regulartimeseries_table = Table('ng_regulartimeseries', metadata, autoload_with=db_3dcitydb)\n",
    "    ng_cityobject_table = Table('ng_cityobject', metadata, autoload_with=db_3dcitydb)\n",
    "    \n",
    "    # obtain the maximum id to insert from the right place\n",
    "    query_3dcitydb = f'''SELECT max(id) FROM {schema_name}.cityobject'''\n",
    "    result_3dcitydb = pd.read_sql(query_3dcitydb, db_3dcitydb)\n",
    "    max_cityobject_id = int(result_3dcitydb.loc[0, 'max'])\n",
    "    print(max_cityobject_id)\n",
    "    \n",
    "    \n",
    "    query_3dcitydb = f'''SELECT * FROM {schema_name}.ng_cityobject'''\n",
    "    existing_ng_cityobject_id = pd.read_sql(query_3dcitydb, db_3dcitydb)\n",
    "\n",
    "\n",
    "    cityobject_to_insert = []\n",
    "    ng_weatherdata_to_insert = []\n",
    "    ng_timeseries_to_insert = []\n",
    "    ng_regulartimeseries_to_insert = []\n",
    "    ng_cityobject_to_insert = []\n",
    "\n",
    "\n",
    "    existing_ids_set = set(existing_ng_cityobject_id['id'])\n",
    "\n",
    "    for idx, row in merged_df.iterrows():\n",
    "        if row['cityobject_id'] not in existing_ids_set:\n",
    "            ng_cityobject_to_insert.append({\n",
    "                'id': row['cityobject_id'],\n",
    "            })\n",
    "\n",
    "        \n",
    "        max_cityobject_id += 1\n",
    "        cityobject_to_insert.append({\n",
    "            'id': max_cityobject_id,\n",
    "            'objectclass_id': 50005,\n",
    "            'gmlid': 'NG_Weather_GlobalSolarIrradiance_UUID_'+str(uuid.uuid4()),\n",
    "            'name': 'Weather_GlobalSolarIrradiance',\n",
    "            'creation_date' : str(datetime.now(timezone.utc).astimezone()),\n",
    "            'last_modification_date' : str(datetime.now(timezone.utc).astimezone()),\n",
    "            'updating_person' : 'postgres'\n",
    "        })\n",
    "    \n",
    "        max_cityobject_id += 1\n",
    "        cityobject_to_insert.append({\n",
    "            'id': max_cityobject_id,\n",
    "            'objectclass_id': 50007,\n",
    "            'gmlid': 'NG_Timeseries_GlobalSolarIrradiance_UUID_'+str(uuid.uuid4()),\n",
    "            'name': 'Timeseries_GlobalSolarIrradiance',\n",
    "            'creation_date' : str(datetime.now(timezone.utc).astimezone()),\n",
    "            'last_modification_date' : str(datetime.now(timezone.utc).astimezone()),\n",
    "            'updating_person' : 'postgres'\n",
    "        })\n",
    "    \n",
    "        ng_timeseries_to_insert.append({\n",
    "            'id': max_cityobject_id,\n",
    "            'objectclass_id': 50033,\n",
    "            'timevaluesprop_acquisitionme' : 'calibratedSimulation',\n",
    "            'timevaluesprop_interpolation' : 'averageInSucceedingInterval'\n",
    "        })\n",
    "        \n",
    "        \n",
    "        ng_regulartimeseries_to_insert.append({\n",
    "            'id': max_cityobject_id,\n",
    "            'timeinterval': 1,\n",
    "            'timeinterval_unit' : 'month',\n",
    "            'values_' : str(np.array(row['monthly_data'], dtype='double').round(3)),\n",
    "            'values_uom' : 'W/m2'\n",
    "            \n",
    "        })\n",
    "    \n",
    "        ng_weatherdata_to_insert.append({\n",
    "            'id': max_cityobject_id-1,\n",
    "            'cityobject_weatherdata_id': row['cityobject_id'],\n",
    "            'values_id' :  max_cityobject_id,\n",
    "            'weatherdatatype' : 'globalSolarIrradiance'\n",
    "        })\n",
    "\n",
    "    print(\"inserting: \",len(ng_regulartimeseries_to_insert))\n",
    "    stmt_cityobject = insert(cityobject_table)\n",
    "    stmt_weatherdata = insert(ng_weatherdata_table)\n",
    "    stmt_timeseries = insert(ng_timeseries_table)\n",
    "    stmt_regulartimeseries = insert(ng_regulartimeseries_table)\n",
    "    stmt_ngcityobject = insert(ng_cityobject_table)\n",
    "    \n",
    "    surface_list = merged_df['cityobject_id'].tolist()\n",
    "    surface_list_str = ', '.join(map(str, surface_list))\n",
    "    \n",
    "    delete_query = f'''\n",
    "    -- Start Transaction\n",
    "    BEGIN;\n",
    "    \n",
    "    -- Create a temporary table to store IDs\n",
    "    CREATE TEMP TABLE temp_ids AS\n",
    "    SELECT id, values_id FROM {schema_name}.ng_weatherdata\n",
    "    WHERE weatherdatatype = 'globalSolarIrradiance' OR weatherdatatype = 'cloudiness'\n",
    "    AND cityobject_weatherdata_id IN ({surface_list_str});\n",
    "    \n",
    "    -- Delete from ng_regulartimeseries\n",
    "    DELETE FROM {schema_name}.ng_regulartimeseries\n",
    "    WHERE id IN (SELECT values_id FROM temp_ids);\n",
    "    \n",
    "    -- Delete from ng_timeseries\n",
    "    DELETE FROM {schema_name}.ng_timeseries\n",
    "    WHERE id IN (SELECT values_id FROM temp_ids);\n",
    "    \n",
    "    -- Delete from ng_weatherdata\n",
    "    DELETE FROM {schema_name}.ng_weatherdata\n",
    "    WHERE id IN (SELECT id FROM temp_ids);\n",
    "    \n",
    "    -- Delete from cityobject based on values_id\n",
    "    DELETE FROM {schema_name}.cityobject\n",
    "    WHERE id IN (SELECT values_id FROM temp_ids);\n",
    "    \n",
    "    -- Delete from cityobject based on id\n",
    "    DELETE FROM {schema_name}.cityobject\n",
    "    WHERE id IN (SELECT id FROM temp_ids);\n",
    "    \n",
    "    -- Drop the temporary table\n",
    "    DROP TABLE temp_ids;\n",
    "    \n",
    "    -- Commit Transaction\n",
    "    COMMIT;\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    print(\"Executing Delete query first to remove relavent data...\")\n",
    "    print(\"It taks a while, around 10 minutes\")\n",
    "    with db_3dcitydb.connect() as conn:\n",
    "        conn.execute(text(delete_query))\n",
    "        print(\"Deleting finished...\")\n",
    "        print(\"Executing INSERT query...\")\n",
    "        if len(ng_cityobject_to_insert)>0:\n",
    "            result = conn.execute(stmt_ngcityobject, ng_cityobject_to_insert)\n",
    "            \n",
    "        result = conn.execute(stmt_cityobject, cityobject_to_insert)\n",
    "        result = conn.execute(stmt_timeseries, ng_timeseries_to_insert)\n",
    "        result = conn.execute(stmt_regulartimeseries, ng_regulartimeseries_to_insert)\n",
    "        result = conn.execute(stmt_weatherdata, ng_weatherdata_to_insert)\n",
    "    \n",
    "        \n",
    "        conn.commit()\n",
    "    \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql+psycopg2://postgres:3344carry@127.0.0.1:5432/solar_calc\n",
      "Connection to database solar_calc was successful\n"
     ]
    }
   ],
   "source": [
    "DB_3DCityDB_ConDetails = \"DB_3DCityDB_ConDetails.txt\"\n",
    "db_3dcitydb = db_tools.engineBuilder(DB_3DCityDB_ConDetails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107200\n",
      "inserting:  21104\n",
      "Executing Delete query first to remove relavent data...\n",
      "It taks a while, around 10 minutes\n",
      "Deleting finished...\n",
      "Executing INSERT query...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "insert_data(target_df, db_3dcitydb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
